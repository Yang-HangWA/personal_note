# Summary of gbdt

#### reading material
- [1.机器学习-一文理解GBDT的原理-20171001](https://zhuanlan.zhihu.com/p/29765582)
- [2.GBDT算法原理深入解析](https://www.zybuluo.com/yxd/note/611571)
- [3.Tree ensemble算法的特征重要度计算](https://www.zybuluo.com/yxd/note/614495)

#### 提升树
- 在说GBDT之前，先说说提升树（boosting tree）。说到提升（boosting），总是绕不过AdaBoost。AdaBoost是利用前一轮迭代的误差率来更新训练集的权重，校正前一轮迭代被错误分类的样本，通俗一点的理解就是将重心放在分错的样本上。提升树也是boosting家族的成员，意味着提升树也采用加法模型（基学习器线性组合）和前向分步算法。下面一个一个进行解释，提升树的基学习器是什么，加法模型和前向分步算法又是怎么用的。提升树通常以决策树作为基学习器，对分类问题决策树是二叉分类树，回归问题就是二叉回归树。

#### GBDT和XGBoost区别
- 传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）；
- 传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；
- XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，放置过拟合，这也是XGBoost优于传统GBDT的一个特性；
- shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）；
列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过 拟合，还能减少计算；
- 对缺失值的处理。对于特征的值有缺失的样本，XGBoost还可以自动 学习出它的分裂方向；
XGBoost工具支持并行。Boosting不是一种串行的结构吗?怎么并行 的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。***XGBoost的并行是在特征粒度上的***。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代 中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。



- RF、GBDT和XGBoost都属于集成学习（Ensemble Learning），集成学习的目的是通过结合多个基学习器的预测结果来改善单个学习器的泛化能力和鲁棒性。
- 根据个体学习器的生成方式，目前的集成学习方法大致分为两大类：即个体学习器之间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表就是Boosting，后者的代表是Bagging和“随机森林”（Random Forest）。

- 1、RF
   - 1.1 原理
        - 提到随机森林，就不得不提Bagging，Bagging可以简单的理解为：放回抽样，多数表决（分类）或简单平均（回归）,同时Bagging的基学习器之间属于并列生成，不存在强依赖关系。
        - Random Forest（随机森林）是Bagging的扩展变体，它在以决策树 为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括RF包括四个部分：
            - 1、随机选择样本（放回抽样）；
            - 2、随机选择特征；
            - 3、构建决策树；
            - 4、随机森林投票（平均）。
        - 随机选择样本和Bagging相同，随机选择特征是指在树的构建中，会从样本集的特征集合中随机选择部分特征，然后再从这个子集中选择最优的属 性用于划分，这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的‘平均’特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。
        - (As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.)
        - 在构建决策树的时候，RF的每棵决策树都最大可能的进行生长而不进行剪枝；在对预测输出进行结合时，RF通常对分类问题使用简单投票法，回归任务使用简单平均法。
        - RF的重要特性是不用对其进行交叉验证或者使用一个独立的测试集获得无偏估计，它可以在内部进行评估，也就是说在生成的过程中可以对误差进行无偏估计，由于每个基学习器只使用了训练集中约63.2%的样本，剩下约36.8%的样本可用做验证集来对其泛化性能进行“包外估计”。
        - `RF和Bagging对比`：RF的起始性能较差，特别当只有一个基学习器时，随着学习器数目增多，随机森林通常会收敛到更低的泛化误差。随机森林的训练效率也会高于Bagging，因为在单个决策树的构建中，Bagging使用的是‘确定性’决策树，在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的是‘随机性’特征数，只需考虑特征的子集。

    - 1.2 优缺点
        - 随机森林的优点较多，简单总结：1、在数据集上表现良好，相对于其他算法有较大的优势（训练速度、预测准确度）；2、能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性；3、容易做成并行化方法。
        - RF的缺点：在噪声较大的分类或者回归问题上回过拟合。

- 2、GBDT
    - 提GBDT之前，谈一下Boosting，Boosting是一种与Bagging很类似的技术。不论是Boosting还是Bagging，所使用的多个分类器类型都是一致的。但是在前者当中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。
    - 由于Boosting分类的结果是基于所有分类器的加权求和结果的，因此Boosting与Bagging不太一样，Bagging中的分类器权值是一样的，而Boosting中的分类器权重并不相等，每个权重代表对应的分类器在上一轮迭代中的成功度。

    - 2.1 原理
        - GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型,所以说，在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting中关注正确错误的样本加权有着很大的区别。
        - 在GradientBoosting算法中，关键就是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树。
        - GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）。

    - 2.2 优缺点
        - GBDT的性能在RF的基础上又有一步提升，因此其优点也很明显，
            - 1、它能灵活的处理各种类型的数据；
            - 2、在相对较少的调参时间下，预测的准确度较高。
    - 当然由于它是Boosting，因此基学习器之前存在串行关系，难以并行训练数据。

- 3、XGBoost
    - 3.1 原理
        - XGBoost的性能在GBDT上又有一步提升，而其性能也能通过各种比赛管窥一二。坊间对XGBoost最大的认知在于其能够自动地运用CPU的多线程进行并行计算，同时在算法精度上也进行了精度的提高。
        - 由于GBDT在合理的参数设置下，往往要生成一定数量的树才能达到令人满意的准确率，在数据集较复杂时，模型可能需要几千次迭代运算。但是XGBoost利用并行的CPU更好的解决了这个问题。
       - 其实XGBoost和GBDT的差别也较大，这一点也同样体现在其性能表现上，详见XGBoost与GBDT的区别。



#### **GBDT VS 随机森林**
- GBDT和随机森林的相同点：
    - 1、都是由多棵树组成
    - 2、最终的结果都是由多棵树一起决定

- GBDT和随机森林的不同点：
    - 1、组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成
    - 2、组成随机森林的树可以并行生成；而GBDT只能是串行生成
    - 3、对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来
    - 4、随机森林对异常值不敏感，GBDT对异常值非常敏感
    - 5、随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成
    - 6、随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能


#### Bagging组合算法

- Bagging组合算法是bootstrap aggregating的缩写。我们可以让上述决策树学习算法训练多轮，每轮的训练集由从初始的训练集中有放回地随机抽取n个训练样本组成，某个初始训练样本在某轮训练集中可以出现多次或根本不出现，训练之后就可以得到一个决策树群h_1,……h_n ，也类似于一个森林。最终的决策树H对分类问题采用投票方式，对回归问题采用简单平均方法对新示例进行判别。



#### Boosting组合算法

- 此类算法中其中应用最广的是AdaBoost(Adaptive Boosting)。在此算法中，初始化时以等权重有放回抽样方式进行训练，接下来每次训练后要特别关注前一次训练失败的训练样本，并赋以较大的权重进行抽样，从而得到一个预测函数序列h_1,⋯, h_m , 其中h_i也有一定的权重，预测效果好的预测函数权重较大，反之较小。最终的预测函数H对分类问题采用有权重的投票方式，所以Boosting更像是一个人学习的过程，刚开始学习时会做一些习题，常常连一些简单的题目都会弄错，但经过对这些题目的针对性练习之后，解题能力自然会有所上升，就会去做更复杂的题目；等到他完成足够多题目后，不管是难题还是简单题都可以解决掉了。



#### 随机森林(Random forest)

- 随机森林，顾名思义，是用随机的方式建立一个森林，所以它对输入数据集要进行行、列的随机采样。行采样采用有放回的随机抽样方式，即采样样本中可能有重复的记录；列采样就是随机抽取部分分类特征，然后使用完全分裂的方式不断循环建立决策树群。当有新的输入样本进入的时候，也要通过投票方式决定最终的分类器。

- 一般的单棵决策树都需要进行剪枝操作，但随机森林在经过两个随机采样后，就算不剪枝也不会出现overfitting。我们可以这样比喻随机森林算法：从M个feature中选择m个让每一棵决策树进行学习时，就像是把它们分别培养成了精通于某一个窄领域的专家，因此在随机森林中有很多个不同领域的专家，对一个新的问题（新的输入数据）可以从不同的角度去看待，最终由各位专家投票得到结果。

- 至此，我们已经简单介绍了各类算法的原理，这些组合算法们看起来都很酷炫。可是它们之间究竟有哪些差异呢？

- 随机森林与Bagging算法的区别主要有两点：
    - 1.随机森林算法会以输入样本数目作为选取次数，一个样本可能会被选取多次，一些样本也可能不会被选取到；而Bagging一般选取比输入样本的数目少的样本；

    - 2.Bagging算法中使用全部特征来得到分类器，而随机森林算法需要从全部特征中选取其中的一部分来训练得到分类器。从我们的实际经验来看，随机森林算法常常优于Bagging算法。

- Boosting和Bagging算法之间的主要区别是取样方式的不同。Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。Bagging和Boosting都可以有效地提高分类的准确性。在多数数据集中，Boosting的准确性比Bagging高一些，不过Boosting在某些数据集中会引起退化——过拟合。

- 俗话说三个臭皮匠赛过诸葛亮，各类组合算法的确有其优越之处；我们也认为，模型效果从好到差的排序通常依次为：随机森林>Boosting > Bagging > 单棵决策树。但归根结底，这只是一种一般性的经验、而非定论，应根据实际数据情况具体分析。就单棵决策树和组合算法相比较而言，决策树相关的组合算法在提高模型区分能力和预测精度方面比较有效，对于像决策树、神经网络这样的“不稳定”算法有明显的提升效果，所以有时会表现出优于单棵决策树的效果。但复杂的模型未必一定是最好的，我们要在具体的分析案例中，根据业务需求和数据情况在算法复杂性和模型效果之间找到平衡点。
